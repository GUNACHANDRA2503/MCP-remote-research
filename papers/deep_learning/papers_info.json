{
  "2306.11113v2": {
    "title": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice",
    "authors": [
      "Deep Pandey",
      "Qi Yu"
    ],
    "summary": "Evidential deep learning, built upon belief theory and subjective logic, offers a principled and computationally efficient way to turn a deterministic neural network uncertainty-aware. The resultant evidential models can quantify fine-grained uncertainty using the learned evidence. To ensure theoretically sound evidential models, the evidence needs to be non-negative, which requires special activation functions for model training and inference. This constraint often leads to inferior predictive performance compared to standard softmax models, making it challenging to extend them to many large-scale datasets. To unveil the real cause of this undesired behavior, we theoretically investigate evidential models and identify a fundamental limitation that explains the inferior performance: existing evidential activation functions create zero evidence regions, which prevent the model to learn from training samples falling into such regions. A deeper analysis of evidential activation functions based on our theoretical underpinning inspires the design of a novel regularizer that effectively alleviates this fundamental limitation. Extensive experiments over many challenging real-world datasets and settings confirm our theoretical findings and demonstrate the effectiveness of our proposed approach.",
    "pdf_url": "https://arxiv.org/pdf/2306.11113v2",
    "published": "2023-06-19"
  },
  "2105.04026v2": {
    "title": "The Modern Mathematics of Deep Learning",
    "authors": [
      "Julius Berner",
      "Philipp Grohs",
      "Gitta Kutyniok",
      "Philipp Petersen"
    ],
    "summary": "We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.",
    "pdf_url": "https://arxiv.org/pdf/2105.04026v2",
    "published": "2021-05-09"
  }
}